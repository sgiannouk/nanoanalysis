###Stavros Giannoukakos### 
#Version of the program
__version__ = "v0.2.1"

import argparse
import subprocess
import glob, sys, os
from pathlib import Path
from datetime import datetime
from collections import Counter


ont_data =  "/home/stavros/playground/ont_basecalling/guppy_v3_basecalling"

refGenomeGRCh38 = "/home/stavros/references/reference_genome/GRCh38_GencodeV31_primAssembly/GRCh38.primary_assembly.genome.fa"
refGenomeGRCh38_traclean = "/home/stavros/references/reference_genome/GRCh38_GencodeV31_primAssembly/GRCh38.primary_assembly.genome.edited.fa"
refAnnot = "/home/stavros/references/reference_annotation/GRCh38_gencode.v35.primary_assembly.annotation.gtf"
refAnnot_bed = "/home/stavros/references/reference_annotation/GRCh38_gencode.v35.primary_assembly.annotation.bed"
rscripts = "{0}/Rscripts".format(os.path.dirname(os.path.realpath(__file__)))
transcriptclean = "python3 /home/stavros/playground/progs/TranscriptClean/TranscriptClean.py"  ### TranscriptClean
sj_ref = "/home/stavros/playground/progs/TranscriptClean/GRCh38_SJs.ref"  ### TranscriptClean ref. SJ
dexseq_prepare_annot = "/home/stavros/R/x86_64-pc-linux-gnu-library/3.6/DEXSeq/python_scripts/dexseq_prepare_annotation.py"
pred_prod = "python3 /home/stavros/playground/progs/flair/bin/predictProductivity.py"


usage = "nanodRNA_analysis [options]"
epilog = " -- June 2019 | Stavros Giannoukakos -- "
description = "DESCRIPTION"

parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, usage=usage, description=description, epilog=epilog)
# Number of threads/CPUs to be used
parser.add_argument('-t', '--threads', dest='threads', default=str(50), metavar='', 
                	help="Number of threads to be used in the analysis")
# Minimum gene counts
parser.add_argument('-mge', '--minGeneExpr', dest='minGeneExpr', default=str(10), metavar='', 
                	help="Min. number of total mapped sequence reads\nfor a gene to be considered expressed")
# Minimum transcript counts
parser.add_argument('-mfe', '--minFeatureExpr', dest='minFeatureExpr', default=str(10), metavar='', 
                	help="Min. number of total mapped sequence reads\nfor a gene isoform to be considered")
# Adjusted p-value threshold for differential expression analysis
parser.add_argument('-adjpval', '--adjPValueThreshold', dest='adjPValueThreshold', default=str(0.05), metavar='', 
                	help="Adjusted p-value threshold for differential\nexpression analysis")
# Minimum required log2 fold change for differential expression analysis
parser.add_argument('-lfc', '--lfcThreshold', dest='lfcThreshold', default=str(2), metavar='', 
                	help="Minimum required log2 fold change for diffe-\nrential expression analysis")
# Top N genes to be used for the heatmap
parser.add_argument('-n', '--n_top', dest='n_top', default=str(100), metavar='', 
                	help="Top N genes to be used for the heatmap")
# Display the version of the pipeline 
parser.add_argument('-v', '--version', action='version', version=f'%(prog)s {__version__}\n{epilog}')
# Get the options and return them
args = parser.parse_args()

script_dir = os.path.dirname(os.path.realpath(__file__))
startTime = datetime.now()

# Main folder hosting the analysis
analysis_dir = os.path.join(script_dir, "analysis_v3.6.1")
prepr_dir = os.path.join(analysis_dir, "preprocessed_data")
alignments_dir = os.path.join(analysis_dir, "alignments")
reports_dir = os.path.join(analysis_dir, "reports")
# Reporting directories
initial_qc_reports = os.path.join(analysis_dir, "reports/initial_qc_reports")
postAlignment_reports = os.path.join(analysis_dir, "reports/post-alignment_qc_reports")
pipeline_reports = os.path.join(analysis_dir, "reports/pipeline_reports")
# Downstream analysis directories
expression_analysis_dir = os.path.join(analysis_dir, "expression_analysis")
R_analysis = os.path.join(analysis_dir, "expression_analysis/R_analysis")
polyA_analysis_dir = os.path.join(analysis_dir, "polyA_estimation")
if not os.path.exists(pipeline_reports): os.makedirs(pipeline_reports)



def quality_control(seq_summary_file, sample_id, raw_data_dir):
	# Producing preliminary QC reports 
	print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} QUALITY CONTROL OF THE INPUT SAMPLES')


	if not os.path.exists(initial_qc_reports): os.makedirs(initial_qc_reports)

	# Unsing NanoPlot (github.com/wdecoster/NanoPlot) to extract basic information regarding the sequencing
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  NanoPlot - Quality Control of the input {sample_id} raw data: in progress ..')
	nanoPlot = " ".join([
	"/usr/local/bin/NanoPlot",  # Call NanoPlot
	"--threads", args.threads,  # Number of threads to be used by the script
	"--summary", seq_summary_file,  # Input of <sequencing_summary> generated by Albacore1.0.0 / Guppy 2.1.3+
	"--color saddlebrown",  # Color for the plots
	"--colormap PuBuGn",  # Colormap for the heatmap
	"--prefix", os.path.join(initial_qc_reports, f"{sample_id}_"),  # Create the report in the reports directory
	"--format png",  # Output format of the plots
	"--dpi 900", # Set the dpi for saving images in high resolution
	"2>>", os.path.join(pipeline_reports, "preprocessing_nanoPlot-report.txt")])
	subprocess.run(nanoPlot, shell=True)
	return

def alignment_against_ref(fastq_pass, sample_id, raw_data_dir, seq_summary_file):
	""" Using Minimap2 to align the raw data against the reference genome """
	print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} ALIGNING AGAINSTE THE REF. GENOME')


	if not os.path.exists(alignments_dir): os.makedirs(alignments_dir)

	### ALIGN THE RAW READS AGAINST THE REFERENCE GENOME
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Minimap2 - Mapping {sample_id} against the reference genome: in progress ..')
	bam_file = os.path.join(alignments_dir, f"{sample_id}.genome.bam")
	minimap2_genome = " ".join([
	"minimap2",  # Call minimap2 (v2.17-r941)
	"-t", args.threads,  # Number of threds to use
	"-ax splice",   # Long-read spliced alignment mode and output in SAM format (-a)
	"-k 13",  # k-mer size
	"-uf",  # Find canonical splicing sites GT-AG - f: transcript strand
	"--secondary=no",  # Do not report any secondary alignments
	"--MD",  # output the MD tag
	# "-o", os.path.join(alignments_dir, "{0}.genome.paf".format(sample_id)),
	refGenomeGRCh38,  # Inputting the reference genome
	fastq_pass,  # Input .fastq.gz file
	"|" "samtools view",
	"--threads", args.threads,  # Number of threads to be used by 'samtools view'
	"-Sb",
	"-q 10",  # Filtering out reads with mapping quality lower than 10
	"|", "samtools sort",  # Calling 'samtools sort' to sort the output alignment file
	"--threads", args.threads,  # Number of threads to be used by 'samtools sort'
	"-",  # Input from standard output
	"-o", bam_file,  # Sorted output  BAM file
	"2>>", os.path.join(pipeline_reports, "alignment_minimap2-report.txt")])  # Directory where all reports reside
	subprocess.run(minimap2_genome, shell=True)
	subprocess.run(f'samtools index -@ {args.threads} {bam_file}', shell=True)
	
	# mapping_qc(sample_id, seq_summary_file, bam_file)
	return

def mapping_qc(sample_id, seq_summary_file, bam_file):
	""" Outputting multiple alignment statistics """
	print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} GENERATING ALIGNMENT STATS')
	

	if not os.path.exists(postAlignment_reports): os.makedirs(postAlignment_reports)

	# BAM stats
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  BAM stats - Generating post-alignment stats of {sample_id}: in progress ..')
	bam_stat = ' '.join([
	"bam_stat.py",
	"-i", bam_file,  # Input BAM file
	f"> {postAlignment_reports}/{sample_id}.bamstat.txt",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_bamstats-report.txt")])
	subprocess.run(bam_stat, shell=True)

	# Picard CollectAlignmentSummaryMetrics
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Picard - Collecting alignment summary metrics of {sample_id}: in progress ..')
	CollectAlignmentSummaryMetrics = ' '.join([
	"picard-tools CollectAlignmentSummaryMetrics",  # Call picard-tools CollectAlignmentSummaryMetrics
	f"INPUT= {bam_file}",  # Input BAM file
	f"OUTPUT= {postAlignment_reports}/{sample_id}.alignment_metrics.txt",  # Output
	f"REFERENCE_SEQUENCE= {refGenomeGRCh38}",  # Reference sequence file
	"2>>", os.path.join(pipeline_reports, "postalignment_collectAlignmentSummaryMetrics-report.txt")])
	subprocess.run(CollectAlignmentSummaryMetrics, shell=True) 

	# # AlignQC 
	# print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  AlignQC - Generating post-alignment stats of {sample_id}: in progress ..')
	# alignqc = ' '.join([
	# "alignqc analyze",  # Calling AlignQC analyze
	# "--threads", str(int(int(args.threads)/2)),
	# "--genome", refGenomeGRCh38,  # Reference in .fasta
	# f"--gtf", "{refAnnot}.gz",  # Input reference annotation in gzipped form
	# f"--output", "{postAlignment_reports}/{sample_id}.alignqc.xhtml",  # Output pdf file
	# bam_file,
	# "2>>", os.path.join(pipeline_reports, "postalignment_alignqc-report.txt")])
	# subprocess.run(alignqc, shell=True)

	# # Wub Alignment based QC plots
	# print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  WUB - Alignment based QC plots of {sample_id}: in progress ..')
	# alignment_qc = ' '.join([
	# "bam_alignment_qc.py",
	# "-f", refGenomeGRCh38,  # Input reference file
	# "-x -Q",  # Do not plot per-reference information/ Keep qiet
	# f"-r", "{postAlignment_reports}/{sample_id}.bam_alignment_qc.pdf",  # Output pdf file
	# f"-p", "{postAlignment_reports}/{sample_id}.bam_alignment_qc.pk",  # Output pk file
	# bam_file,
	# "2>>", os.path.join(pipeline_reports, "postalignment_wub-report.txt")])
	# subprocess.run(alignment_qc, shell=True)

	### EXPORTING ALIGNMENT STATS
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  PycoQC - Generating post-alignment stats of {sample_id}: in progress ..')
	pycoQC = " ".join([
	"pycoQC",  # Call pycoQC
	"--summary_file", seq_summary_file, 
	"--bam_file", bam_file,  # Input of bam files from Minimap2
	"--html_outfile", os.path.join(postAlignment_reports, f"{sample_id}.pycoQC-report.html"),  # Create the report in the reports directory
	"--report_title", "\"Post-alignment quality control report\"",  # A title to be used in the html report
	"2>>", os.path.join(pipeline_reports, "postalignment_pycoQC-report.txt")])
	subprocess.run(pycoQC, shell=True)

	# BAM read distribution
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  RSeQC - Generating read distribution stats of {sample_id}: in progress ..')
	read_distribution = ' '.join([
	"read_distribution.py", 
	"-i", bam_file,  # Input BAM file
	"-r", refAnnot_bed,
	f"> {postAlignment_reports}/{sample_id}.fragSize",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_read_distribution-report.txt")])
	subprocess.run(read_distribution, shell=True)

	# Check the strandness of the reads
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  RSeQC - Generating read strandness stats of {sample_id}: in progress ..')
	strandness = ' '.join([
	"infer_experiment.py",  # Call samtools infer_experiment
	"-i", bam_file,  # Input BAM file
	"-r", refAnnot_bed,
	f"> {postAlignment_reports}/{sample_id}.strandness.txt",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_strandness-report.txt")])
	subprocess.run(strandness, shell=True)

	# Gene body coverage
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  RSeQC - Generating gene body coverage of {sample_id}: in progress ..')
	gene_coverage = ' '.join([
	"geneBody_coverage.py",  # Call samtools geneBody_coverage
	"-i", bam_file,  # Input BAM file
	"-r", refAnnot_bed,
	f"-o", "{postAlignment_reports}/gene_coverage.{sample_id}",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_gene_coverage-report.txt")])
	subprocess.run(gene_coverage, shell=True)

	# Check duplicate reads
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Picard - Extracting read duplication stats of {sample_id}: in progress ..')
	duplicate_reads = ' '.join([
	"picard-tools MarkDuplicates",  # Call MarkDuplicates
	f"INPUT= {bam_file}",  # Input BAM file
	f"OUTPUT= {alignments_dir}/{sample_id}.genome.dedup.bam",
	f"METRICS_FILE= {postAlignment_reports}/{sample_id}.mark_duplicates.txt",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_picard_markDuplicate-report.txt")])
	subprocess.run(duplicate_reads, shell=True)
	subprocess.run(f'rm {alignments_dir}/{sample_id}.genome.dedup.bam', shell=True)

	# Number of reads mapped to each chromosome
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  RSeQC - Generating mapping stats of {sample_id}: in progress ..')
	mapping_pos = ' '.join([
	"samtools idxstats",  # Call samtools idxstats
	bam_file,  # Input BAM file
	f"> {postAlignment_reports}/{sample_id}.samtools_idxstats.txt",  # Output file
	"2>>", os.path.join(pipeline_reports, "postalignment_samtools_idxstats-report.txt")])
	subprocess.run(mapping_pos, shell=True)
	return

def polyA_estimation(sample_id, sum_file, fastq_pass, raw_data_dir):
	""" PolyA length estimation using Nanopolish polyA """
	print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} POLYA LENGTH ESTIMATION')


	polyA_analysis_dir_idv = os.path.join(polyA_analysis_dir, sample_id)
	if not os.path.exists(polyA_analysis_dir_idv): os.makedirs(polyA_analysis_dir_idv)

	# Extracting the *pass.fastq.gz file 
	extracted_fastq = os.path.join(polyA_analysis_dir_idv, os.path.basename(fastq_pass)[:-3])
	subprocess.run(f'gzip --decompress --keep --stdout {fastq_pass} > {extracted_fastq}', shell=True)
	
	# Nanopolish index
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Nanopolish index - Indexing the output of the guppy basecaller of sample {sample_id}: in progress ..')
	indexing = ' '.join([
	"nanopolish index",  # Calling Nanopolish index
	"--sequencing-summary", sum_file,  # the sequencing summary file from Guppy
	"--directory", f"{raw_data_dir}/workspace/fast5_pass",  # Input BAM file
	extracted_fastq,
	"2>>", os.path.join(pipeline_reports, "nanopolish_index-report.txt")])
	subprocess.run(indexing, shell=True)

	# Nanopolish polyA
	print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Nanopolish polyA -  Estimate the polyadenylated tail lengths of {sample_id}: in progress ..')
	polyA_est = ' '.join([
	"nanopolish polya",  # Calling Nanopolish polyA
	"--threads", args.threads,  # Number of threads to use
	"--genome", refGenomeGRCh38,  # The reference genome assembly that  was used
	"--reads", extracted_fastq,  # The raw 1D ONT direct RNA reads in fastq
	"--bam", os.path.join(alignments_dir, f"{sample_id}.genome.bam"),  # The reads aligned to the genome assembly in BAM format
	f"> {polyA_analysis_dir_idv}/{sample_id}.polya_results.tsv",  # Output file
	"2>>", os.path.join(pipeline_reports, "nanopolish_polyA-report.txt")])
	subprocess.run(polyA_est, shell=True)

	### Removing unnecessary files
	subprocess.run(f'rm {extracted_fastq}', shell=True)  # Removing the extracted fastq
	subprocess.run(f'rm {polyA_analysis_dir}/*/fastq_runid*', shell=True)  # Removing index 
	return

class expression_analysis:

	def __init__(self):
		self.talon_analysis()
		self.talon_visualisation()
		return

	def talon_analysis(self):
		""" TALON takes transcripts from one or more long read datasets (SAM format) 
		and assigns them transcript and gene identifiers based on a database-bound
		annotation. Novel events are assigned new identifiers """
		print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} ANNOTATION AND QUANTIFICATION USING TALON')
		

		talon_database = os.path.join(expression_analysis_dir, 'talon_gencode_v35.db')  ### TALON DB
		if not os.path.exists(R_analysis): os.makedirs(R_analysis)
		temp = os.path.join(expression_analysis_dir, 'transcriptClean_temp')
		if not os.path.exists(temp): os.makedirs(temp)
		filtered_isoforms_final = os.path.join(expression_analysis_dir, "filtered_isoforms_final.csv")
		reference_fasta = os.path.join(expression_analysis_dir, "reference_transcriptome.fasta")

		sample_group = []
		# Create config file that is needed for talon and converting the aligned bam files to sam
		csv_file = os.path.join(expression_analysis_dir,'talon_input.csv')
		if not os.path.exists(csv_file) or os.stat(csv_file).st_size == 0:
			print("{0}  Creating a description file necessary for Talon: in progress ..".format(datetime.now().strftime("%d.%m.%Y %H:%M")))
			with open(csv_file, "w") as talon_out:
				for path, subdir, folder in os.walk(alignments_dir):
					for file in sorted(folder):
						if file.endswith('.genome.bam'):
							sample_group.append(file.split(".")[0].split("_")[0])
							# Output a csv file with 'sample_id, sample_group, technology, input_labeled_sam_file' which is gonna be needed in talon_annotation function
							talon_out.write('{0},{1},ONT,{2}\n'.format(file.split(".")[0], file.split(".")[0].split("_")[0], os.path.join(temp, file).replace(".genome.bam", ".clean_labeled.sam")))
							if not os.path.exists(os.path.join(path, file).replace(".bam", ".sam")):
								print("{0}  Samtools - Converting the genomic bam file ({1}) to sam for Talon: in progress ..".format(datetime.now().strftime("%d.%m.%Y %H:%M"), file))
								os.system('samtools view -h -@ {0} {1} > {2}'.format(args.threads, os.path.join(path, file), os.path.join(path, file).replace(".bam", ".sam")))


		### Initial step: Correcting mismatches, microindels, and noncanonical splice junctions in long reads that have been mapped to the genome
		sam_files = glob.glob(os.path.join(alignments_dir, "*.genome.sam"))
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")} 1/10 TranscriptClean - Correcting mismatches, microindels, and noncanonical splice junctions in long reads: in progress ..')
		for file in sam_files:
			TranscriptClean = " ".join([
			transcriptclean,  # Call TranscriptClean.py
			"--sam", file,  # Input sam file
			"--genome", refGenomeGRCh38_traclean,  # Reference genome fasta file
			"--threads", args.threads,  # Number of threads to be used by the script
			"--canonOnly",  # Output only canonical transcripts and transcripts containing annotated noncanonical junctions to the clean SAM file
			"--spliceJns", sj_ref,  # Splice junction file extracted from the ref. GTF
			"--deleteTmp",  # the temporary directory (TC_tmp) will be removed
			"--outprefix", os.path.join(temp, os.path.basename(file).split(".")[0]),  # Outprefix for the outout file
			"2>>", os.path.join(pipeline_reports, "talon1_transcriptclean-report.txt")])  # Directory where all reports reside
			subprocess.run(TranscriptClean, shell=True)
			os.remove(file)
		
		### Second step: Building the reference database
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")} 2/10 TALON - Initiating the database: in progress ..')
		if os.path.exists(talon_database): os.remove(talon_database)
		original = sys.stdout
		sys.stdout = open(f'{pipeline_reports}/talon2_initialize_database-report.txt',"w")
		talon_initialize_database = " ".join([
		"talon_initialize_database",  # Call talon_initialize_database
		"--f", refAnnot,  # GTF annotation containing genes, transcripts, and edges
		"--g", 'hg38',  # Genome build (i.e. hg38) to use
		"--5p 500",  # Maximum allowable distance (bp) at the 5' end during annotation
		"--3p 300",  # Maximum allowable distance (bp) at the 3' end during annotation
		"--a", talon_database[:-3],  # Name of supplied annotation
		"--o", talon_database[:-3],  # Outprefix for the annotation files
		"2>>", os.path.join(pipeline_reports, "talon2_initialize_database-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_initialize_database, shell=True)
		sys.stdout = original

		### Third step: internal priming check
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  3/10 TALON - Run talon_label_reads on each file to compute how likely each read is to be an internal priming product: in progress ..')
		genome_alignments = [sum_file for sum_file in glob.glob(os.path.join(temp, "*clean.sam"))]
		original = sys.stdout
		sys.stdout = open(f'{pipeline_reports}/talon3_talon_priming_check-report.txt',"w")
		for file in genome_alignments:
			talon_priming_check = " ".join([
			"talon_label_reads",  # Call talon talon_label_reads
			"--t", args.threads,  # Number of threads to be used by the script
			"--f", file,  # Input sam file
			"--g", refGenomeGRCh38,  # Reference genome fasta file
			"--tmpDir", os.path.join(expression_analysis_dir, "tmp_label_reads"),  # Path to directory for tmp files
			"--deleteTmp",  # Temporary directory will be removed
			"--o", os.path.join(temp, os.path.basename(file).replace("_clean.sam",".clean")),  # Prefix for output files
			"2>>", os.path.join(pipeline_reports, "talon3_talon_priming_check-report.txt")])  # Directory where all reports reside
			subprocess.run(talon_priming_check, shell=True)
		sys.stdout = original

		### Fourth step: annotating and quantification of the reads
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  4/10 TALON - Annotating and quantification of the reads: in progress ..')
		original = sys.stdout
		sys.stdout = open(f'{pipeline_reports}/talon4_annotationNquantification-report.txt',"w")
		talon_annotation = " ".join([
		"talon",  # Call talon
		"--threads", args.threads,  # Number of threads to be used by the script
		"--db", talon_database,  # TALON database
		"--build", 'hg38',  # Genome build (i.e. hg38) to use
		"--o", os.path.join(expression_analysis_dir, "prefilt"),  # Prefix for output files
		"--f", csv_file,  # Dataset config file: dataset name, sample description, platform, sam file (comma-delimited)
		"2>>", os.path.join(pipeline_reports, "talon4_annotationNquantification-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_annotation, shell=True)
		sys.stdout = original

		### Fifth step: summarising how many of each transcript were found (prior to any filtering)
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  5/10 TALON - Summarising how many of each transcript were found (prior to any filtering): in progress ..')
		talon_summary = " ".join([
		"talon_summarize",  # Call talon_summarize
		"--db", talon_database,  # TALON database
		"--o", os.path.join(expression_analysis_dir, "prefilt"),  # Prefix for output file
		"2>>", os.path.join(pipeline_reports, "talon5_summary-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_summary, shell=True)
		
		### Sixth step: creating an abundance matrix without filtering (for use computing gene expression)
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  6/10 TALON - Creating an abundance matrix without filtering (gene expression): in progress ..')
		talon_abundance = " ".join([
		"talon_abundance",  # Call talon_abundance
		"--db", talon_database,  # TALON database
		"--build", 'hg38',  # Genome build (i.e. hg38) to use
		"--annot", talon_database[:-3],  # Which annotation version to use
		"--o", os.path.join(expression_analysis_dir, "prefilt"),  # Prefix for output file
		"2>>", os.path.join(pipeline_reports, "talon6_abundance-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_abundance, shell=True)

		### Seventh step: Applying basic filtering steps and outputting several stats
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  7/10 TALON - Removing internal priming artifacts low abundance transcripts: in progress ..')
		talon_filter = " ".join([
		"talon_filter_transcripts",  # Call talon_filter_transcripts
		"--db", talon_database,  # TALON database
		"--annot", talon_database[:-3],  # Which annotation version to use
		"--minCount 5",  # Number of minimum occurrences required for a novel transcript PER dataset
		"--maxFracA 0.5",  # All of the supporting reads must have 50% or fewer As in the 20 bp interval after alignment
		"--minDatasets", str(min(Counter(sample_group).values())),  # Minimum number of datasets novel transcripts must be found in
		"--o", os.path.join(expression_analysis_dir, "filtered_isoforms.csv"),  # Output
		"2>>", os.path.join(pipeline_reports, "talon7_talon_filter-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_filter, shell=True)
		
		### Eighth step:
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  8/10 TALON - Applying custom filtering of the ISM group based on the polyA estimation: in progress ..')
		prefit_abundance_matrix = os.path.join(expression_analysis_dir, "prefilt_talon_abundance.tsv")
		prefilt_readannot_matrix = os.path.join(expression_analysis_dir, "prefilt_talon_read_annot.tsv")
		filtered_isoforms = os.path.join(expression_analysis_dir, "filtered_isoforms.csv")
		self.polyA_filtering(prefit_abundance_matrix, prefilt_readannot_matrix, filtered_isoforms, filtered_isoforms_final)
		
		### Ninth step: Applying basic filtering steps and outputting several stats
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  9/10 TALON - Removing filtered isoforms based on step 8 and exporting basic statsistics (ISM filters included): in progress ..')
		talon_filter_n_report = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/talon_summarisation.R",  # Calling the talon_summarisation.R script
		os.path.join(expression_analysis_dir, "prefilt_talon_abundance.tsv"),  # Input matrix
		R_analysis,  # Output directory
		os.path.join(expression_analysis_dir, "talon_input.csv"),  # Input annotation matrix
		os.path.join(expression_analysis_dir, "filtered_isoforms_final.csv"),  # Filtered transcripts to maintain, including ISM filtering
		os.path.join(expression_analysis_dir, "filtered_isoforms.csv"),  # Filtered transcripts to maintain, original
		"2>>", os.path.join(pipeline_reports, "talon9_summarisation.txt")])  # Directory where all reports reside
		subprocess.run(talon_filter_n_report, shell=True)
		
		### Tenth step: Generating TALON report for each dataset
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  10/10 TALON - Generating TALON report for each dataset (talon filters only): in progress ..')
		for file in glob.glob(os.path.join(alignments_dir, "*.genome.bam")):
			sample_name = os.path.basename(file).split(".")[0]
			output_dir = os.path.join(R_analysis, f"talon_reports/{sample_name}_report")
			if not os.path.exists(output_dir): os.makedirs(output_dir)
			talon_report = " ".join([
			"talon_generate_report",  # Call talon_abundance
			"--db", talon_database,  # TALON database
			"--whitelists", os.path.join(expression_analysis_dir, "filtered_isoforms.csv"),  # Filtered transcripts to be reported
			"--datasets", sample_name,  # Input of the filtered tables produced on the previous step
			"--outdir", output_dir,  # Output dir
			"2>>", os.path.join(pipeline_reports, "talon10_generate_report-report.txt")])  # Directory where all reports reside
			subprocess.run(talon_report, shell=True)

		# ### Removing unnecessary directories and files
		subprocess.run(f"rm -r {temp}", shell=True)  
		subprocess.run("rm -r talon_tmp", shell=True)
		self.extract_talon_database(talon_database, reference_fasta)  # Extract Talon database in gtf and fasta format
		return

	def polyA_filtering(self, prefilt_talon_abundance, talon_read_annot, filtered_isoforms, output_file):
		""" Here is where all the magic of the special filtering is taking place. 
		Here we are using the information of the polyA length estimation to filter 
		out novel transcripts in the ISM category """
		
		not_to_keep = {}
		filtered_isolist = {}
		with open(filtered_isoforms) as filtin:
			for line in filtin:
				filtered_isolist[line.strip().split(",")[1]] = line.strip().split(",")[0]

		prefiltered_transcripts = {}
		with open(prefilt_talon_abundance) as filt_tr:
			for line in filt_tr:
				if not line.startswith("gene_ID"):
					if line.strip().split("\t")[1] in filtered_isolist:
						prefiltered_transcripts[line.strip().split("\t")[3]] = line.strip().split("\t")[1]

		read_annot_dict = {}
		# TALON read annotation file and saving it as a dictionary 
		with open(talon_read_annot) as read_annot:
			for line in read_annot:
				if line.strip().split("\t")[12] in prefiltered_transcripts:
					read = line.strip().split("\t")[0]
					sample = line.strip().split("\t")[1]
					transcript_id = line.strip().split("\t")[12]
					read_type = "{0}_{1}_{2}".format(prefiltered_transcripts[transcript_id],line.strip().split("\t")[16],line.strip().split("\t")[17])
					if "ISM" in read_type:  # Removing the ISM Suffix group from the downstream analysis. This will be analysed differently
						read_annot_dict[(sample, read)] = (transcript_id, read_type)
					# elif "ISM_Suffix" in read_type:
					# 	not_to_keep[prefiltered_transcripts[transcript_id]] = None


		### Iterating through all "polya_results.tsv" obtained from the Nanopolish polyA function 
		### in order to filter out transcripts in the ISM Prefix group without enough polyA evidence
		filter_dict = {}
		for path, subdir, folder in os.walk(analysis_dir):
			for name in folder:
				if name.endswith("polya_results.tsv"):
					sample = name.split(".")[0]
					polyA_length_est = os.path.join(path, name)
					with open(polyA_length_est) as fin:
						for line in fin:
							if not line.startswith("readname"):
								readname = line.strip().split("\t")[0]
								if (sample ,readname) in read_annot_dict:
									transcript_name = read_annot_dict[(sample ,readname)][0] 
									read_type = read_annot_dict[(sample ,readname)][1]
									# if "ISM_Suffix" in read_type:print(read_type)
									qc = ["OTHER","PASS"][line.strip().split("\t")[-1]=="PASS"]
									transcript_ID = read_type.split("_")[0]
									if "ISM" in read_type:
										if (transcript_name, transcript_ID) in filter_dict:
											if qc ==  "PASS":
												filter_dict[(transcript_name, transcript_ID)][0] += 1
											else:
												filter_dict[(transcript_name, transcript_ID)][1] += 1
										else:
											if qc ==  "PASS":
												filter_dict[(transcript_name, transcript_ID)] = [1,0]
											else:
												filter_dict[(transcript_name, transcript_ID)] = [0,1]
		###621266_ISM_Suffix
		# for a,b in filter_dict.items():
		# 	print(a,b)

		for transcript, qc_tags in filter_dict.items():	# Obtaining the list with the transcripts where the PASS is
			if qc_tags[0] < qc_tags[1]:						# less than the rest
				not_to_keep[transcript[1]] = None

		# If not_to_keep list is empty, raise a warning..
		if len(not_to_keep) == 0: print("WARNING: all ISM transcripts passed the polyA QC filter!")

		with open(output_file, "w") as fout:			# Rewriting the "filtered_isoforms.csv" excluding the
			for trID, gnID in filtered_isolist.items():	# filtered transcripts of the ISM group without enough
				if not trID in not_to_keep:			# polyA evidence.
					fout.write(f"{gnID},{trID}\n")
		return

	def extract_talon_database(self, talon_database, reference_fasta):

		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  Obtaining the transcriptome annotation from the TALON database: in progress ..')
		talon_export_db = " ".join([
		"talon_create_GTF",  # Call talon_create_GTF
		"--db", talon_database,  # TALON database
		"--build hg38",  # Genome build (hg38) to use
		"--annot", talon_database[:-3],  # Which annotation version to use
		"--o", f"{expression_analysis_dir}/database",  # Output
		"--whitelist", f"{expression_analysis_dir}/final_filtered_isoforms_for_db.csv",  # Whitelist file of transcripts to include in the output
		"2>>", os.path.join(pipeline_reports, "talonextract_exportdb-report.txt")])  # Directory where all reports reside
		subprocess.run(talon_export_db, shell=True)

		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")} Converting TALON database to fasta format: in progress ..')
		convert_db = " ".join([
		"gffread",  # Call gffread
		"--sort-alpha",  # chromosomes (reference sequences) are sorted alphabetically
		"-g", refGenomeGRCh38,  # Fasta file with the genomic sequences for all input mappings
		"-w", reference_fasta,  # Write a fasta file with spliced exons for each transcript
		f"{expression_analysis_dir}/database_talon.gtf",  # Input gtf
		"2>>", os.path.join(pipeline_reports, "talonextract_convertdb-report.txt")])  # Directory where all reports reside
		subprocess.run(convert_db, shell=True)
		return

	def talon_visualisation(self):
		
		annot = {}
		with open(os.path.join(expression_analysis_dir, "database_talon.gtf")) as ref_in:
			for i, line in enumerate(ref_in):
				if not line.startswith("#"):
					if line.split("\t")[2].strip() == 'transcript':
						transcript_id = line.split("\t")[-1].split(";")[1].split(" ")[-1].strip("\"")
						transcript_type = ["novel", line.split("\t")[-1].split(";")[4].split()[1].strip("\"").strip()][transcript_id.startswith("ENST")]
						if transcript_type.endswith("pseudogene"):
							transcript_type = "pseudogene"
						elif transcript_type in ["Mt_rRNA","miRNA","snoRNA","misc_RNA","snRNA","scaRNA","rRNA","Mt_tRNA"]:
							transcript_type = "ncRNA"
						elif transcript_type == "ribozyme" or transcript_type.startswith("IG_") or transcript_type.startswith("TR_"):
							transcript_type = "protein_coding"
						elif transcript_type == "TEC":
							transcript_type = "To_be_Confirmed"
						annot[transcript_id] = transcript_type

						
		data = {}
		header = []
		with open(f"{expression_analysis_dir}/filt_talon_abundance.csv") as mat_in:
			for i, line in enumerate(mat_in, 1):
				if i == 1:
					header = line.strip().split(",")[9:]
				else:
					transcript = line.strip().split(",")[1]
					values = line.strip().split(",")[9:]
					data[(transcript, annot[transcript])] = values

		# Remove paths from sample names
		header.insert(0,"transcript_id")  # Inserting ttranscript_id in header
		header.insert(1, "transcript_type")  # Inserting transcript_type in header
		
		# Writing output to file 'expression_matrix.csv'
		with open(f"{expression_analysis_dir}/perTranscript_expression_matrix.csv", "w") as fout:
			fout.write("{0}\n".format(','.join(header)))
			for key, values in data.items():
				fout.write("{0},{1}\n".format(','.join(key), ','.join(values)))

		print("{0}  Visualising the RNA categories found in the TALON expression matrix: in progress ..".format(datetime.now().strftime("%d.%m.%Y %H:%M")))
		gene_type_sum = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/transcript_type_summary.R",  # Calling the transcript_type_summary.R script
		f"{expression_analysis_dir}/perTranscript_expression_matrix.csv",  # Input matrix
		R_analysis,  # Output dir
		"2>>", os.path.join(pipeline_reports, "R_transcriptType_sum-report.txt")])  # Directory where all reports reside
		subprocess.run(gene_type_sum, shell=True)
		return

class downstream_analysis:

	def __init__(self):
		# self.polyA_length_est_analysis()
		self.differential_expression_analysis()
		# self.methylation_detection()
		return

	def polyA_length_est_analysis(self):
		""" Here is where all the magic of the special filtering is taking place. 
		Here we are using the information of the polyA length estimation to filter 
		out novel transcripts in the ISM category. We are also performing the 
		differential polyadelylation analysis (DPA) """
		print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} POLYA LENGTH ESTIMATION ANALYSIS')
		


		filtered_transcripts = {}
		filt_talon_abundance = os.path.join(expression_analysis_dir,"filt_talon_abundance.csv")
		with open(filt_talon_abundance) as filt_tr:
			for line in filt_tr:
				if not line.startswith("annot_gene_id"):
					filtered_transcripts[line.strip().split(",")[1]] = None

		read_annot_dict = {}
		# TALON read annotation file and saving it as a dictionary 
		talon_read_annot = os.path.join(expression_analysis_dir,"prefilt_talon_read_annot.tsv")
		with open(talon_read_annot) as read_annot:
			for line in read_annot:
				if line.strip().split("\t")[12] in filtered_transcripts:
					read = line.strip().split("\t")[0]
					sample = line.strip().split("\t")[1]
					transcript_id = line.strip().split("\t")[12]
					gene_id = line.strip().split("\t")[11]
					read_type = "{0}_{1}".format(line.strip().split("\t")[16], line.strip().split("\t")[17])
					read_annot_dict[(sample, read)] = (transcript_id, gene_id, read_type)

		for path, subdir, folder in os.walk(analysis_dir):
			for name in folder:
				if name.endswith("polya_results.tsv"):
					sample = name.split(".")[0]
					print(sample, datetime.now().strftime("%H:%M"))
					polyA_length_est = os.path.join(path, name)
					with open(polyA_length_est) as fin, open(polyA_length_est.replace(".tsv",".transcripts.tsv"), "w") as transcript_out:
						 # open(polyA_length_est.replace(".tsv",".genes.tsv"), "w") as gene_out, \
						for i, line in enumerate(fin, 1):
							if i == 1:
								# gene_out.write("{0}\n".format(line.strip()))
								transcript_out.write("{0}\n".format(line.strip()))
							else:
								readname = line.strip().split("\t")[0]
								if (sample ,readname) in read_annot_dict:
									# gene_id = read_annot_dict[(sample ,readname)][1]
									transcript_id = read_annot_dict[(sample ,readname)][0] 
									read_type = read_annot_dict[(sample ,readname)][2]
									rest = "\t".join(line.strip().split("\t")[3:])
									# gene_out.write(f"{readname}\t{gene_id}\t{read_type}\t{rest}\n")
									transcript_out.write(f"{readname}\t{transcript_id}\t{read_type}\t{rest}\n")
					
					# Writing basic info to 'polyA_data_info' for NanoTail analysis in  transcript level
					sample_info_transcripts = "{0}/polyA_transcript_info.csv".format(polyA_analysis_dir)
					with open(sample_info_transcripts, "a") as fout_tr:
						fout_tr.write("{0},{1},{2}/{0}/{0}.polya_results.transcripts.tsv\n".format(sample, sample.split("_")[0], polyA_analysis_dir))
					# nanotail_analysis(sample_info_transcripts, "transcript")  # Transcript level analysis
		return

	def nanotail_analysis(self, sample_info, what):
		### Running Nanotail analysis
		print(f'{datetime.now().strftime("%d.%m.%Y %H:%M")}  NanoTail - Differential polyadenilation analysis: in progress ..')
		nanotail_analysis = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/polyA_analysis.R",  # Calling the polyA_analysis.R script
		sample_info,  # Input annotation
		os.path.join(polyA_analysis_dir, f"{what}_analysis"),  # Output directory
		"2>>", os.path.join(pipeline_reports, "downstream_nanotail-report.txt")])  # Directory where all reports reside
		subprocess.run(nanotail_analysis, shell=True)
		return

	def differential_expression_analysis(self):
		print(f'\n\t{datetime.now().strftime("%d.%m.%Y %H:%M")} DIFFERENTIAL EXPRESSION ANALYSIS')
		
		
		### First step: Exploratory analysis
		print(f'\n{datetime.now().strftime("%d.%m.%Y %H:%M")}  1/4 Differential Expression - Exploratory analysis: in progress ..')
		expl_analysis = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/diffExpr_ExplAnalysis.R",  # Calling the diffExpr_ExplAnalysis.R script
		os.path.join(expression_analysis_dir, "prefilt_talon_abundance.tsv"),  # Input filtered matrix
		os.path.join(expression_analysis_dir, "talon_input.csv"),  # Input annotation matrix
		R_analysis,  # Output directory
		args.minGeneExpr,  # minGeneExpr - Minimum number of reads for a gene to be considered expressed
		args.n_top,  # Top n_top genes for creating the heatmap
		"2>>", os.path.join(pipeline_reports, "diffExpr_exploratory_analysis-report.txt")])  # Directory where all reports reside
		subprocess.run(expl_analysis, shell=True)
		

		### Second step: DGE
		print(f'\n{datetime.now().strftime("%d.%m.%Y %H:%M")}  2/4 Differential Expression - Differential Gene Expression (DGE) analysis using DRIMSeq/edgeR: in progress ..')
		dge_analysis = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/diffExpr_DGE.R",  # Calling the diffExpr_DGE.R script
		os.path.join(expression_analysis_dir, "prefilt_talon_abundance.tsv"),  # Input filtered matrix
		os.path.join(expression_analysis_dir, "talon_input.csv"),  # Input annotation matrix
		R_analysis,  # Output directory
		args.minGeneExpr,  # minGeneExpr - Minimum number of reads for a gene to be considered expressed
		args.adjPValueThreshold,  # adjPValueThreshold - Adjusted p-value threshold for differential expression
		args.lfcThreshold,  # lfcThreshold - Minimum required log2 fold change for differential expression
		"2>>", os.path.join(pipeline_reports, "diffExpr_dge_analysis-report.txt")])  # Directory where all reports reside
		subprocess.run(dge_analysis, shell=True)
		

		### Third step: DTE
		print(f'\n{datetime.now().strftime("%d.%m.%Y %H:%M")}  3/4 Differential Expression - Differential Transcript Expression (DTE) analysis using DRIMSeq/edgeR: in progress ..')
		dte_analysis = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/diffExpr_DTE.R",  # Calling the diffExpr_DTE.R script
		os.path.join(expression_analysis_dir, "filt_talon_abundance.csv"),  # Input filtered matrix
		os.path.join(expression_analysis_dir, "prefilt_talon_read_annot.tsv"),  # Read annotation matrix
		os.path.join(expression_analysis_dir, "talon_input.csv"),  # Input annotation matrix
		R_analysis,  # Output directory
		args.minFeatureExpr,  # minFeatureExpr - Minimum number of reads for a gene isoform to be considered
		args.adjPValueThreshold,  # adjPValueThreshold - Adjusted p-value threshold for differential expression
		args.lfcThreshold,  # lfcThreshold - Minimum required log2 fold change for differential expression		
		"2>>", os.path.join(pipeline_reports, "diffExpr_dte_analysis-report.txt")])  # Directory where all reports reside
		subprocess.run(dte_analysis, shell=True)


		### Fourth step: DTU
		print(f'\n{datetime.now().strftime("%d.%m.%Y %H:%M")}  4/4 Differential Expression - Differential Transcript Usage (DTU) analysis using IsoformSwitchAnalyzeR: in progress ..')
		dtu_analysis = " ".join([
		"Rscript",  # Call Rscript
		f"{rscripts}/diffExpr_DTU.R",  # Calling the diffExpr_DTU.R script
		os.path.join(expression_analysis_dir, "filt_talon_abundance.csv"),  # Input filtered matrix
		os.path.join(expression_analysis_dir, "talon_input.csv"),  # Input annotation matrix
		R_analysis,  # Output directory
		f"{expression_analysis_dir}/reference_transcriptome.fasta",  # Fasta file with spliced exons for each transcript
		f"{expression_analysis_dir}/database_talon.gtf",  # Transcriptome annotation from the TALON database
		# "2>>", os.path.join(pipeline_reports, "diffExpr_dtu_analysis-report.txt")
		])  # Directory where all reports reside
		subprocess.run(dtu_analysis, shell=True)
		return 

	def methylation_detection(self):
		if not os.path.exists(methylation_dir): os.makedirs(methylation_dir)
		single_fast5_data = [os.path.dirname(sum_file) for sum_file in glob.glob("/shared/projects/silvia_rna_ont_umc/basecalling/sample*_raw_data/*/")]

		# print(single_fast5_data)
		for dirs in single_fast5_data:
			if dirs.endswith("/0"):

				sample_id = dirs.split("/")[5].split("_")[0]
				# 1. Re-squiggling the raw reads
				tombo_resquiggle = " ".join([
				"tombo resquiggle",
				dirs,
				refGenomeGRCh38,
				"--processes", args.threads,  # Number of threads to be used
				"--num-most-common-errors 5",
				# "2>>", os.path.join(pipeline_reports, "tombo_resquiggle-report.txt")
				])
				subprocess.run(tombo_resquiggle, shell=True)
			
			# 2. Calling tombo to do the methylation analysis
			tombo_methyl = " ".join([
			"tombo detect_modifications de_novo",  # Indexing the concat_samples.bam file
			"--fast5-basedirs", dirs,  # Directory containing fast5 files
			"--statistics-file-basename", os.path.join(methylation_dir,"{0}.tombo.stats".format(sample_id)),
			"--rna",  # Explicitly select canonical RNA mode
			"--processes", args.threads,  # Number of threads to be used
			# "2>>", os.path.join(pipeline_reports, "tombo_methylation-report.txt")
			])
			subprocess.run(tombo_methyl, shell=True)

			# 3. Output reference sequence around most significantly modified sites
			tombo_sign = " ".join([
			"tombo text_output signif_sequence_context",  # Indexing the concat_samples.bam file
			"--fast5-basedirs", dirs,  # Directory containing fast5 files
			"--statistics-filename", os.path.join(methylation_dir,"{0}.tombo.stats".format(sample_id)),
			"--sequences-filename",  os.path.join(methylation_dir,"{0}.tombo_significant_regions.fasta".format(sample_id)),
			# "2>>", os.path.join(pipeline_reports, "tombo_significants-report.txt")
			])
			subprocess.run(tombo_sign, shell=True)

			# # 4. Use line Meme to estimate modified motifs
			# tombo_meme = " ".join([
			# "meme",
	  #  		"-rna",
	  #  		"-mod zoops", 
			# "-oc", os.path.join(methylation_dir,"{0}_de_novo_meme".format(sample_id)),
	  #  		os.path.join(methylation_dir,"{0}.tombo_significant_regions.fasta".format(sample_id)),
	  #  		# "2>>", os.path.join(pipeline_reports, "tombo_meme-report.txt")
			# ])
			# subprocess.run(tombo_meme, shell=True)

			# # 5. This plot will identify the sites in the reference
			# tombo_plot = " ".join([
			# "tombo plot motif_with_stats",
			# " --fast5-basedirs", dirs,
	  #  		"--motif CCWGG",
	  #  		"--genome-fasta", refGenomeGRCh38,
	  #  		"--statistics-filename", os.path.join(methylation_dir,"{0}.tombo.stats".format(sample_id)),
	  #  		"--pdf-filename", os.path.join(methylation_dir,"{0}.tombo.pdf".format(sample_id))
	  #  		# "2>>", os.path.join(pipeline_reports, "tombo_plot-report.txt")
			# ])
			# subprocess.run(tombo_plot, shell=True)
		return

def summary():
	
	multiQC = " ".join([
	"multiqc",  # Call MultiQC
	"--quiet",  # Print only log warnings
	"--outdir", postAlignment_reports,  # Create report in the FastQC reports directory
	"--filename", "post-alignment_summarised_report",  # Name of the output report 
	postAlignment_reports,  # Directory where all FastQC and Cutadapt reports reside
	"2>>", os.path.join(pipeline_reports, "post-alignment_multiQC-report.txt")])  # Output multiQC report
	subprocess.run(multiQC, shell=True)

	pickle_files = [pk_file for pk_file in glob.glob(os.path.join(postAlignment_reports, "*_qc.pk"))]
	### Wub Compare alignment QC statistics of multiple samples
	bam_multi_qc = ' '.join([
	"bam_multi_qc.py",
	"-r", "{0}/comparison_qc.pdf".format(postAlignment_reports),  # Output pdf file
	' '.join(pickle_files),
	"2>>", os.path.join(pipeline_reports, "bam_multi_qc-report.txt")])
	# subprocess.run(bam_multi_qc, shell=True)

	## Cleaning up reports folder
	figures_pre_dir = os.path.join(initial_qc_reports, "individual_plots")
	if not os.path.exists(figures_pre_dir): os.makedirs(figures_pre_dir)

	os.system('mv {0}/*.png {1}'.format(initial_qc_reports, figures_pre_dir))  # Moving png files to "figures" directory
	
	## REMOVING UNNECESSARY FILES & REPORTS (postanalysis)
	for path, subdir, folder in os.walk(pipeline_reports):
		for name in folder:
			file = os.path.join(path, name)
			if os.stat(file).st_size == 0 or\
			(name.endswith("multiQC-report.txt") and os.stat(file).st_size == 583) or\
			(name.endswith("bamstats-report.txt") and os.stat(file).st_size == 24) or\
	  		(name.endswith("ribution-report.txt") and os.stat(file).st_size == (255 or 256)) or\
	  		(name.endswith("Metrics-report.txt") and os.stat(file).st_size == 3) or\
	  		(name.endswith("bamstats-report.txt") and os.stat(file).st_size == 24) or\
	  		(name.endswith("R_transcriptType_sum-report.txt") and os.stat(file).st_size == 210) or\
	  		(name.endswith("diffExpr_exploratory_analysis-report.txt") and os.stat(file).st_size == 3648) or\
	  		(name.endswith("diffExpr_dge_analysis-report.txt") and os.stat(file).st_size == 347) or\
	  		(name.endswith("strandness-report.txt") and os.stat(file).st_size == 205 or os.stat(file).st_size == 214) or\
	  		 name.endswith(".r") or name.endswith("DupRate.xls") or name.endswith("duplicate_reads-report.txt"):
				os.remove(file)

	## Cleaning up postanalysis folder
	individual_postal_reports = os.path.join(postAlignment_reports, "individual_reports")
	if not os.path.exists(individual_postal_reports): os.makedirs(individual_postal_reports)

	# Moving files to "qc_reports" directory
	# os.system('mv {0}/*_qc.pk {1}'.format(postAlignment_reports, qc_reports))
	os.system('mv {0}/*.txt {1}'.format(postAlignment_reports, individual_postal_reports))
	os.system('mv {0}/*.fragSize {1}'.format(postAlignment_reports, individual_postal_reports))
	return


def main():
	

	chosen_samples = ("NonTransf_1",  "NonTransf_2",  "NonTransf_3", "Tumour_1",  "Tumour_2",  "Tumour_4")
	summary_files = [str(file_path) for file_path in Path(ont_data).glob('**/sequencing_summary.txt') if not "warehouse" in str(file_path)]
	num_of_samples = len(summary_files)

	for sum_file in [s for s in summary_files if os.path.dirname(s).endswith(chosen_samples)]:
		raw_data_dir = os.path.dirname(str(sum_file))
		sample_id = os.path.basename(raw_data_dir)
		fastq_pass = " ".join(glob.glob(os.path.join(raw_data_dir, "pass/*pass.fastq.gz")))
		# print(f'\nPROCESSING SAMPLE {sample_id}')

		# quality_control(sum_file, sample_id, raw_data_dir)

		# alignment_against_ref(fastq_pass, sample_id, raw_data_dir, sum_file)
			
		# polyA_estimation(sample_id, sum_file, fastq_pass, raw_data_dir)

	expression_analysis()

	downstream_analysis()

	summary()

	print(f'\t--- The pipeline finisded after {datetime.now() - startTime} ---')

if __name__ == "__main__": main()